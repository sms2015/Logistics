{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load training, train, validation, and test sets for X\n",
    "training_X = pd.read_pickle('D:/CUNY Files/capstone/code files/training_X_v.pkl').values\n",
    "train_X = pd.read_pickle('D:/CUNY Files/capstone/code files/train_X_v.pkl').values\n",
    "validation_X = pd.read_pickle('D:/CUNY Files/capstone/code files/validation_X_v.pkl').values\n",
    "test_X = pd.read_pickle('D:/CUNY Files/capstone/code files/test_X_v.pkl').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load training, train, validation, and test sets for y\n",
    "training_y = pd.read_pickle('D:/CUNY Files/capstone/code files/training_y_v.pkl').values\n",
    "train_y = pd.read_pickle('D:/CUNY Files/capstone/code files/train_y_v.pkl').values\n",
    "validation_y = pd.read_pickle('D:/CUNY Files/capstone/code files/validation_y_v.pkl').values\n",
    "test_y = pd.read_pickle('D:/CUNY Files/capstone/code files/test_y_v.pkl').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 721, 181, 226)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_X), len(train_X), len(validation_X), len(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 721, 181, 226)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_y), len(train_y), len(validation_y), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of features\n",
    "n_features = train_X.shape[1]\n",
    "n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Define the baseline model **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a keras model and evaluate it with with scikit-learn.\n",
    "\n",
    "Define a baseline model with a single fully connected hidden layer with the same number of neurons as input attributes (features). The network uses the rectifier activation function ('relu')for the hidden layer. No activation function is used for the output layer because it is a regression problem and we are interested in predicting numerical values directly. \n",
    "\n",
    "The optimization algorithm used is: ADAM \n",
    "The loss function optimized is: mean squared error \n",
    "This MSE will be also used to evaluate the performance of the model. Using this metric and taking the square root provides an error value that can be easily interpreted in the context of the problem which is the number of days between the estimated arrival date and actual arrival date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_features, input_dim=n_features, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 15\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardized the features:  \n",
    "Use scikit-learn Pipeline function to perform the standardization during the model evaluation process, within each fold of the cross validation. This ensures that there is no data leakage from each testset cross validation fold into the training data.\n",
    "\n",
    "The cross validation provides an estimate of the modelâ€™s performance for unseen data. The result reports the mean squared error including the average and standard deviation (average variance) across all 10 folds of the cross validation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 40.08 (19.87) MSE\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, \n",
    "                                         epochs=5, batch_size=10,verbose =0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, training_X, training_y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an improvement over the polynomial ridge regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Evaluate a Deeper Network Topology  **  \n",
    "  \n",
    "Test if adding another layer will improve the performance of the neural network. (This might allow the model to extract and recombine higher order features embedded in the data.)  \n",
    "  \n",
    "Add an additional layer with 15 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a model with one additional layer over the baseline model\n",
    "def deeper_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_features, input_dim=n_features, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(15, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deeper: 39.75 (19.28) MSE\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "estimators_d = []\n",
    "estimators_d.append(('standardize', StandardScaler()))\n",
    "estimators_d.append(('mlp', KerasRegressor(build_fn=deeper_model, \n",
    "                                         epochs=5, batch_size=10, verbose=0)))\n",
    "pipeline_d = Pipeline(estimators_d)\n",
    "kfold_d = KFold(n_splits=10, random_state=seed)\n",
    "results_d = cross_val_score(pipeline_d, training_X, training_y, cv=kfold_d)\n",
    "print(\"Deeper: %.2f (%.2f) MSE\" % (results_d.mean(), results_d.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is essentially the same as the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Evaluate a Wider Network Topology **\n",
    "\n",
    "Test if adding more neurons will improve the performance of the neural network.  Maintain a shallow network but substantially increase the number of neurons in the one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neurons equal to 150% of the number of features (inputs)\n",
    "n_features_200 = int(round(n_features * 2,0))\n",
    "n_features_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_features_200, input_dim=n_features, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wider: 39.89 (19.52) MSE\n"
     ]
    }
   ],
   "source": [
    "#this takes about 10 minutes to run (11 seconds per epoch, 20 k-folds)\n",
    "np.random.seed(seed)\n",
    "estimators_w = []\n",
    "estimators_w.append(('standardize', StandardScaler()))\n",
    "estimators_w.append(('mlp', KerasRegressor(build_fn=wider_model, \n",
    "                                         epochs=5, batch_size=10, \n",
    "                                           verbose=0)))\n",
    "pipeline_w = Pipeline(estimators_w)\n",
    "kfold_w = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline_w, training_X, training_y, cv=kfold_w)\n",
    "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results are no better than the baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit the baseline model\n",
    "pipeline.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "filename = 'neural_net.sav'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
